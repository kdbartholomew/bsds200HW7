# -*- coding: utf-8 -*-
"""bsdshw7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ETxlL9RKnvhL5Cl1hpWJyJrl730Tom0S
"""

#kaggle titanic random forest// hw project for bsds200
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import sklearn as sk
import sklearn.model_selection
import sklearn.ensemble

df_labeled = pd. read_csv('/content/drive/MyDrive/spaceship-titanic/train.csv')
df_test = pd.read_csv('/content/drive/MyDrive/spaceship-titanic/test.csv')#get tianic data from kaggle page, #ypred== 1 gives true or false , maximize score

df_train, df_val = sk.model_selection.train_test_split(df_labeled)

df_train.info()
df_train.dtypes

df_test.info()

#inspection:
# counts the number of null values per column
null_counts = df_train.isnull().sum()
print(null_counts)
#we will have to impute values for all positive sums !
#HomePlanet: impute mode???? its a label not a number

# df train

impute_vals = {'HomePlanet': df_train['HomePlanet'].mode(), 'Destination': df_train['Destination'].mode(), 'Age': df_train['Age'].mean(), 'CryoSleep': df_train['CryoSleep'].mode(), 'Cabin': df_train['Cabin'].mode(), 'VIP': df_train['VIP'].mode(),'RoomService': df_train['RoomService'].mean(), 'FoodCourt': df_train['FoodCourt'].mean(), 'ShoppingMall': df_train['ShoppingMall'].mean(), 'Spa': df_train['Spa'].mean(), 'VRDeck': df_train['VRDeck'].mean() }

#df test
impute_vals_test = {
    'HomePlanet': df_test['HomePlanet'].mode(),
    'Destination': df_test['Destination'].mode(),
    'Age': df_test['Age'].mean(),
    'CryoSleep': df_test['CryoSleep'].mode(),
    'Cabin': df_test['Cabin'].mode(),
    'VIP': df_test['VIP'].mode(),
    'RoomService': df_test['RoomService'].mean(),
    'FoodCourt': df_test['FoodCourt'].mean(),
    'ShoppingMall': df_test['ShoppingMall'].mean(),
    'Spa': df_test['Spa'].mean(),
    'VRDeck': df_test['VRDeck'].mean()
}

#selecting the cols were training on... feature engineering

columns = ['Age', 'CryoSleep', 'Cabin', 'Destination', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'VIP']
#columns = ['Age', 'CryoSleep', 'Cabin', 'Destination', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']
X_train = df_train[columns] #we are sleecting the columns we want to consider
X_val = df_val[columns]

#actually imputing values:
X_train = X_train.fillna(impute_vals)
X_val = X_val.fillna(impute_vals)

#changing non-numerics
X_train['CryoSleep'] = (df_train['CryoSleep'] == 'FALSE').astype('float') #if they werent asleep they had a better chance of surving
X_val['CryoSleep'] = (df_val['CryoSleep'] == 'FALSE').astype('float')
#does this^ fix true also? yes!
X_train['VIP'] = (df_train['VIP'] == 'FALSE').astype('float') #if they werent asleep they had a better chance of surving
X_val['VIP'] = (df_val['VIP'] == 'FALSE').astype('float')

#change age to if child or not? or do i do this when i spit the node? idk whats happening on the backend
  #i think when i give it age it splits in the way that minimizes mse, so unecessary, might cause overfitting to consider both

#one hot encoding: converting categoricals to binary cols
#change cabin
X_train['Cabin'] = X_train['Cabin'].str[0] #gets the first element in the string, which is the deck
X_train = pd.get_dummies(X_train, columns=['Cabin']) #one hot encoding step: converts the categorical variable Cabin into multiple binary columns, each representing one category (in this case, the first letter of the cabin).
for col in X_train.columns:
    if col.startswith('Cabin_'):
        X_train[col] = X_train[col].astype(float)
#does the same as above for x_val!
X_val['Cabin'] = X_val['Cabin'].str[0]
X_val = pd.get_dummies(X_val, columns=['Cabin'])
for col in X_val.columns:
    if col.startswith('Cabin_'):
        X_val[col] = X_val[col].astype(float)

#change destination
X_train = pd.get_dummies(X_train, columns=['Destination']) #one hot encoding step: converts the categorical variable Cabin into multiple binary columns, each representing one category (in this case, the first letter of the cabin).
for col in X_train.columns:
    if col.startswith('Destination_'):
        X_train[col] = X_train[col].astype(float)
#same shit for x_val
X_val = pd.get_dummies(X_val, columns=['Destination'])
for col in X_val.columns:
    if col.startswith('Destination_'):
        X_val[col] = X_val[col].astype(float)

#df test

# Selecting the columns we are considering
columns = ['Age', 'CryoSleep', 'Cabin', 'Destination', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'VIP']

# Selecting the relevant columns from df_test
X_test = df_test[columns]

# Actually imputing values
X_test = X_test.fillna(impute_vals_test)

# Changing non-numerics
X_test['CryoSleep'] = (X_test['CryoSleep'] == 'FALSE').astype('float')
X_test['VIP'] = (X_test['VIP'] == 'FALSE').astype('float')

# Change cabin
X_test['Cabin'] = X_test['Cabin'].str[0]
X_test = pd.get_dummies(X_test, columns=['Cabin'])
for col in X_test.columns:
    if col.startswith('Cabin_'):
        X_test[col] = X_test[col].astype(float)

# Change destination
X_test = pd.get_dummies(X_test, columns=['Destination'])
for col in X_test.columns:
    if col.startswith('Destination_'):
        X_test[col] = X_test[col].astype(float)

null_counts = df_test[columns].isnull().sum()
print(null_counts)

X_train.info()
null_counts = df_train[columns].isnull().sum()
print(null_counts)

y_train = df_train['Transported'] #mismatch bc X_train now contains cabins
y_val = df_val['Transported']

X_train.info()
X_train.isnull().sum()

X_val.info()
X_val.isnull().sum()

tree = sk.tree.DecisionTreeClassifier()
tree.fit(X_train, y_train)
y_pred = tree.predict(X_val)

#implementing random forest
clf = sk.ensemble.RandomForestClassifier(n_estimators = 100)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_val)

#implementing gradient boosting
from xgboost import XGBClassifier
clf = XGBClassifier(
    n_estimators = 200,
    max_depth = 3,
    learning_rate = .1,
    sub_sample = .8,
    colsample_bytree = .8
)

clf.fit(X_train, y_train)
y_pred = clf.predict(X_val)
#user warning?? WARNING: /workspace/src/learner.cc:742: Parameters: { "sub_sample" } are not used.

acc_val = np.mean(y_pred == y_val)
print(f'acc_val is: {acc_val}')

#vip made marginal improvement

# !git clone https://github.com/kdbartholomew/bsds200HW7.git
# #

# %cd bsds200HW7

X_test

# !git add .
# !git commit -m "slay"
# !git push origin master  # or main, depending on your branch name

#gpt:
# Make predictions on the test data

y_test_pred = clf.predict(X_test)  # Assuming 'clf' is your trained model
#y_test_pred = (y_test_pred == 'True').astype(int) #converting bacl to true false for submission format!


# Create a DataFrame with PassengerId and predictions
submission_df = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Transported': y_test_pred})


submission_df['Transported'] = submission_df['Transported'].replace({0: 'False', 1: 'True'})
# Save the DataFrame as a CSV file
submission_df.to_csv('submission.csv', index=False)

submission_df.head()

#score of 0.0 but acc of 79%

#x test has been mutated in the same way as xtrain
#should i only be looking at transported for xtest?


#can i get the pca code? i need help understanding that shit also